{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Lexicons and Distributional Semantics\n",
    "\n",
    "This is due on **Friday, 11/10 (11pm)** \n",
    "\n",
    "## How to do this problem set\n",
    "\n",
    "Most of these questions require writing Python code and computing results, and the rest of them have textual answers.  Write all the textual answers in this document, show the output of your experiment in this document, and implement the functions in the `python` files. \n",
    "\n",
    "Submit a PDF of thie .ipynb to Gradescope, and the .ipynb and all python files to Moodle.\n",
    "\n",
    "The assignment has two parts:\n",
    " * In the first part, you will experiment with Turney's method to find word polarities in a twitter dataset, given some positive and negative seed words.\n",
    " * In the second part, you will experiment with distributional and vector semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Name: Shivangi Singh **\n",
    "\n",
    "**List collaborators, and how you collaborated, here:** (see our [grading and policies page](http://people.cs.umass.edu/~brenocon/inlp2017/grading.html) for details on our collaboration policy).\n",
    "\n",
    "* Pritish Yuvraj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Lexicon semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall that PMI of a pair of words, is defined as:\n",
    "\n",
    "$$PMI(x, y) = log\\frac{ P(x, y) }{ P(x)P(y)}$$\n",
    "\n",
    "The Turney mehod defines a word's polarity as:\n",
    "\n",
    "$$Polarity(word) = PMI(word, positive\\_word)−PMI(word, negative\\_word)$$\n",
    "\n",
    "where the joint probability $P(w, v)$ or, more specifically, $P(w\\ NEAR\\ v)$ is the probability of both being \"near\" each other.  We'll work with tweets, so it means: if you choose a tweet at random, what's the chance it contains both `w` and `v`?\n",
    "\n",
    "(If you look at the Turney method as explained in the SLP3 chapter, the \"hits\" function is a count of web pages that contain at least one instance of the two words occurring near each other.)\n",
    "\n",
    "The positive_word and negative_word terms are initially constructed by hand. For example: we might start with single positive word ('excellent') and a single negative word ('bad'). We can also have list of positive words ('excellent', 'perfect', 'love', ....) and list of negative words ('bad', 'hate', 'filthy',....)\n",
    "\n",
    "If we're using a seed list of multiple terms, just combine them into a single symbol, e.g. all the positive seed words get rewritten to POS_WORD (and similarly for NEG_WORD).  This $P(w, POS\\_WORD)$ effectively means the co-ocurrence of $w$ with any of the terms in the list.\n",
    "\n",
    "For this assignment, we will use twitter dataset which has 349378 tweets. These tweets are in the file named `tweets.txt`. These are the tweets of one day and filtered such that each tweet contains at least one of the seed words we've selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (15 points)\n",
    "\n",
    "The file `tweets.txt` contains around 349,378 tweets with one tweet per line.  It is a random sample of public tweets, which we tokenized with [twokenize.py's tokenizeRawTweetText()](https://github.com/myleott/ark-twokenize-py/blob/master/twokenize.py)). The text you see has a space between each token so you can just use `.split()` if you want.  We also filtered tweets to ones that included at least one term from one of these seed lists:\n",
    "* Positive seed list: [\"good\", \"nice\", \"love\", \"excellent\", \"fortunate\", \"correct\", \"superior\"]\n",
    "* Negative seed list: [\"bad\", \"nasty\", \"poor\", \"hate\", \"unfortunate\", \"wrong\", \"inferior\"]\n",
    "\n",
    "Each tweet contains at least one positive or negative seed word. Take a look at the file (e.g. `less' and `grep'). Implement the Turney's method to calculate polarity scores of all words.\n",
    "\n",
    "Some things to keep in mind:\n",
    "* Ignore the seed words (i.e. don't calculate the polarity of the seed words).\n",
    "* You may want to ignore the polarity of words beignning with `@` or `#`. \n",
    "\n",
    "We recommend that you write code in a python file, but it's up to you.\n",
    "\n",
    "QUESTION: You'll have to do something to handle zeros in the PMI equation. Please explain your and justify your decision about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Laplace Smoothing and add 1 (pseudocount) to counts of (w,pos/neg) and (w) if it doesnt exist so that our denominator is not 0.\n",
    "\n",
    "\n",
    "Also the implementation is done in the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 2 (5 points)\n",
    "\n",
    "Print the top 50 most-positive words (i.e. inferred positive words) and the 50 most-negative words.\n",
    "\n",
    "Many of the words won't make sense.  Comment on at least two that do make sense, and at least two that don't.  Why do you think these are happening with this dataset and method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 NEG WORDS: \n",
      "(0, 'Saf\\xe2\\x80\\xa6')\n",
      "(1, 'https://t.co/ELGdDnERnv')\n",
      "(2, 'they\\\\')\n",
      "(3, 'Primarily')\n",
      "(4, 'multi\\xe2\\x80\\xa6')\n",
      "(5, 'jdjxjcjszjsj\\\\')\n",
      "(6, 'cracker')\n",
      "(7, 'https://t.co/gEonTHTjFz')\n",
      "(8, 'n-word\\\\')\n",
      "(9, 'fliers')\n",
      "(10, 'pretext')\n",
      "(11, '16\\xe2\\x80\\xa6')\n",
      "(12, 'safest')\n",
      "(13, 'Worse')\n",
      "(14, 'Vague')\n",
      "(15, 'https://t.co/fit4as5b\\xe2\\x80\\xa6')\n",
      "(16, 'forms\\\\')\n",
      "(17, '-Scott')\n",
      "(18, 'farmer')\n",
      "(19, 'Stressing')\n",
      "(20, 'Lakh')\n",
      "(21, 'Commercial')\n",
      "(22, 'idea\\xe2\\x80\\xa6')\n",
      "(23, 'Twitte\\xe2\\x80\\xa6')\n",
      "(24, 'ploughing')\n",
      "(25, 'vandalism')\n",
      "(26, 'anti-Muslim')\n",
      "(27, '\\xe2\\x86\\x91')\n",
      "(28, 'https://t.co/mzZUXnzsif')\n",
      "(29, 'https://t.co/J3riaV1YRB')\n",
      "(30, 'collapsing')\n",
      "(31, 'https://t.co/8ZHa4A\\xe2\\x80\\xa6')\n",
      "(32, 'hearts\\\\')\n",
      "(33, 'am\\xc3\\xa9n')\n",
      "(34, 'https://t.co/P3gF70ys33')\n",
      "(35, '\\xf0\\x9f\\x98\\xb1\\xf0\\x9f\\x99\\x8f')\n",
      "(36, '30Days')\n",
      "(37, '*197%*')\n",
      "(38, 'Commerce')\n",
      "(39, \"O'Care:\")\n",
      "(40, 'https://t.co/VnpBSk5net')\n",
      "(41, 'https://t.co/guLHi9TCH1')\n",
      "(42, 'JCCs')\n",
      "(43, 'centers')\n",
      "(44, '\\xe2\\x80\\x9caccess\\xe2\\x80\\x9d')\n",
      "(45, 'defeating')\n",
      "(46, 'https://t.co/28pnrYgiXl')\n",
      "(47, 'eradicate')\n",
      "(48, 'Tw\\xe2\\x80\\xa6')\n",
      "(49, 'removes')\n",
      "\n",
      "50 POS WORDS: \n",
      "(1, 'Teyana')\n",
      "(2, 'Von')\n",
      "(3, 'Herban')\n",
      "(4, 'https://t.co/I46MhN5tag')\n",
      "(5, 'Keef')\n",
      "(6, '\\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x98\\x8d')\n",
      "(7, 'https://t.co/inU4PQoS7z')\n",
      "(8, 'shrek')\n",
      "(9, 'compatibility')\n",
      "(10, 'https://t.co/FQZq3jAFc8')\n",
      "(11, 'Calculated')\n",
      "(12, '\\xe2\\x99\\xa1')\n",
      "(13, 'doghouse')\n",
      "(14, 'doggo\\xe2\\x80\\xa6')\n",
      "(15, 'pupared')\n",
      "(16, 'Meera')\n",
      "(17, 'colores')\n",
      "(18, 'paredes')\n",
      "(19, 'https://t.co/80KoJB2TyM')\n",
      "(20, 'arguing\\\\')\n",
      "(21, 'RP2')\n",
      "(22, 'https://t.co/dDCjkLBNJR')\n",
      "(23, 'hosting')\n",
      "(24, '\\xf0\\x9f\\x98\\x8d\\xe2\\x9d\\xa4')\n",
      "(25, 'BIT\\xe2\\x80\\xa6')\n",
      "(26, 'https://t.co/huNoGXIknz')\n",
      "(27, 'only\\xf0\\x9f\\x8d\\x80')\n",
      "(28, 'https://t.co/LNGO5TDPH6')\n",
      "(29, 'https://t.co/2q2UbkBmZc')\n",
      "(30, 'memes\\xe2\\x80\\xa6')\n",
      "(31, 'https://t.co/xDmus3YIv5')\n",
      "(32, 'https://t.co/tWWOT4T9Xh')\n",
      "(33, 'Cuddling')\n",
      "(34, 'https://t.co/10h74ZBeja')\n",
      "(35, '\\xf0\\x9f\\x92\\x98')\n",
      "(36, 'https://t.co/Su15suSG6v')\n",
      "(37, 'mla')\n",
      "(38, '*7')\n",
      "(39, 'lik\\xe2\\x80\\xa6')\n",
      "(40, 'Greater')\n",
      "(41, '\\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x95\\xba\\xf0\\x9f\\x8f\\xbd')\n",
      "(42, '\\xf0\\x9f\\x98\\x8dThey')\n",
      "(43, 'https://t.co/O95oqHkyOJ')\n",
      "(44, '\\xf0\\x9f\\x98\\xac\\xf0\\x9f\\x92\\xaf')\n",
      "(45, 'this\\xf0\\x9f\\x98\\x8a')\n",
      "(46, '\\xf0\\x9f\\x99\\x84\\xf0\\x9f\\x99\\x84\\xf0\\x9f\\xa4\\xb7\\xf0\\x9f\\x8f\\xbd\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f')\n",
      "(47, 'treated\\xe2\\x80\\xa6')\n",
      "(48, '\\xf0\\x9f\\x8c\\xb4')\n",
      "(49, 'Records')\n",
      "(50, 'https://t.co/HL8PP7BFHO')\n"
     ]
    }
   ],
   "source": [
    "# Write code to print words here\n",
    "from collections import defaultdict\n",
    "import math \n",
    "import operator\n",
    "\n",
    "posword_dic= defaultdict(float)\n",
    "negword_dic= defaultdict(float)\n",
    "totalword_dic= defaultdict(float)\n",
    "pos_seed=[\"good\", \"nice\", \"love\", \"excellent\", \"fortunate\", \"correct\", \"superior\"]\n",
    "neg_seed=[\"bad\", \"nasty\", \"poor\", \"hate\", \"unfortunate\", \"wrong\", \"inferior\"]\n",
    "dic_PMI = defaultdict(float)\n",
    "\n",
    "def match_2_lists(list1, list2):\n",
    "    for i in list1:\n",
    "        if i in list2:\n",
    "            return 1 \n",
    "    return 0 \n",
    "\n",
    "def cal_counts():\n",
    "    a= file (\"/Users/shivangisingh/Downloads/hw3/tweets.txt\",'r')\n",
    "    # This is doing the counts for the whole file\n",
    "    for sentence in a:\n",
    "        x=sentence.split() # x is a list of tokenized words\n",
    "        if match_2_lists(x, pos_seed):\n",
    "            for i in x:\n",
    "                if (i[0]!='.' and i[0]!='#'and i[0]!='@'and (i not in neg_seed) and (i not in pos_seed)):\n",
    "                    posword_dic[i] += 1 \n",
    "        \n",
    "        if match_2_lists(x, neg_seed):\n",
    "            for y in x:\n",
    "                if (y[0]!='.' and y[0]!='#' and y[0]!='@'and (y not in neg_seed) and (y not in pos_seed)):\n",
    "                    negword_dic[y]+=1\n",
    "                    \n",
    "#             iterate over x and add it to the positive dictionary\n",
    "        for y in x:\n",
    "            if (y[0]!='.' and y[0]!='#'and y[0]!='@'and (y not in neg_seed) and (y not in pos_seed)):\n",
    "                totalword_dic[y]+=1\n",
    "    #print posword_dic.keys()\n",
    "    \n",
    "def calculate_PMI():\n",
    "    pos = sum(posword_dic.values())\n",
    "    neg = sum(posword_dic.values())\n",
    "    for k in totalword_dic:            \n",
    "#     At this stage you have the various counts and you have to find the PMI for the two words.\n",
    "        pos_PMI=math.log(float(posword_dic[k])+1/float((totalword_dic[k]+1)*pos))\n",
    "        neg_PMI=math.log(float(negword_dic[k])+1/float((totalword_dic[k]+1)*neg))\n",
    "        polarity_w= pos_PMI-neg_PMI\n",
    "        dic_PMI[k]=polarity_w\n",
    "\n",
    "cal_counts()\n",
    "#print posword_dic#, negword_dic, totalword_dic\n",
    "#'''\n",
    "calculate_PMI()\n",
    "sorted_PMI = sorted(dic_PMI.items(), key=operator.itemgetter(1))\n",
    "print(\"50 NEG WORDS: \")\n",
    "for i in range(0,50):\n",
    "    print (i,sorted_PMI[i][0])\n",
    "# print(sorted_PMI[0:50])\n",
    "print\n",
    "print(\"50 POS WORDS: \")\n",
    "for i in range(1,51):\n",
    "    print (i,sorted_PMI[-i][0])\n",
    "# print(sorted_PMI[-50:])\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Textual answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on at least two that do make sense, and at least two that don't. Why do you think these are happening with this dataset and method?\n",
    "\n",
    "\n",
    "safest, Lakh and they as the most negative word doesn't make sense. defeating and vandelisim does make sense to belong in the most negative list. Cannot say much about the positive list because it looks like most of them are emoticons, and links to websites.\n",
    "So people in this dataset might be sharing links to websites with positive intentions/ seed-words. \n",
    "Also, the algorithm is being bias towards infrequent words in a seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (5 points)\n",
    "\n",
    "Now filter out all the words which have total count < 500, and then print top 50 polarity words and bottom 50 polarity words. \n",
    "\n",
    "Choose some of the words from both the sets of 50 words you got above which accoording to you make sense. Again please note, you will find many words which don't make sense. Do you think these results are better than the results you got in Question-1? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 NEG WORDS Filtered on count >500: \n",
      "(0, 'Saf\\xe2\\x80\\xa6')\n",
      "(1, 'CHILD')\n",
      "(2, 'cigarettes')\n",
      "(3, 'condemning')\n",
      "(4, 'crimes')\n",
      "(5, 'united')\n",
      "(6, 'marijuana')\n",
      "(7, 'I\\xe2\\x80\\x99ll')\n",
      "(8, 'stands')\n",
      "(9, 'forms')\n",
      "(10, 'beer')\n",
      "(11, 'ME')\n",
      "(12, 'explain')\n",
      "(13, 'store')\n",
      "(14, 'crime')\n",
      "(15, 'evil')\n",
      "(16, 'ugly')\n",
      "(17, 'pick')\n",
      "(18, 'Too')\n",
      "(19, 'Why')\n",
      "(20, 'against')\n",
      "(21, 'after')\n",
      "(22, 'women')\n",
      "(23, 'Dems')\n",
      "(24, 'w')\n",
      "(25, 'stand')\n",
      "(26, 'went')\n",
      "(27, 'American')\n",
      "(28, 'men')\n",
      "(29, 'negative')\n",
      "(30, 'ppl')\n",
      "(31, 'parents')\n",
      "(32, 'its')\n",
      "(33, 'whole')\n",
      "(34, 'country')\n",
      "(35, 'Trump')\n",
      "(36, 'says')\n",
      "(37, 'ask')\n",
      "(38, 'every')\n",
      "(39, 'send')\n",
      "(40, '--')\n",
      "(41, 'myself')\n",
      "(42, 'must')\n",
      "(43, 'use')\n",
      "(44, 'something')\n",
      "(45, 'woman')\n",
      "(46, 'believe')\n",
      "(47, 'going')\n",
      "(48, 'What')\n",
      "(49, 'once')\n",
      "\n",
      "50 POS WORDS Filtered on count >500: \n",
      "(1, 'Teyana')\n",
      "(2, 'Von')\n",
      "(3, 'Keef')\n",
      "(4, 'https://t.co/I46MhN5tag')\n",
      "(5, 'Herban')\n",
      "(6, 'https://t.co/inU4PQoS7z')\n",
      "(7, '\\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x98\\x8d')\n",
      "(8, 'Decay')\n",
      "(9, 'ol')\n",
      "(10, 'Taylor')\n",
      "(11, 'hood')\n",
      "(12, 'pregnancy')\n",
      "(13, 'Happy')\n",
      "(14, 'birthday')\n",
      "(15, '\\xe2\\x9d\\xa4\\xef\\xb8\\x8f')\n",
      "(16, '\\xe2\\x9d\\xa4')\n",
      "(17, 'D')\n",
      "(18, 'hilarious')\n",
      "(19, '\\xf0\\x9f\\x98\\x8a')\n",
      "(20, '\\xf0\\x9f\\x92\\x95')\n",
      "(21, 'morning')\n",
      "(22, 'Thank')\n",
      "(23, 'Be')\n",
      "(24, 'fall')\n",
      "(25, 'thank')\n",
      "(26, 'thanks')\n",
      "(27, 'falling')\n",
      "(28, 'beautiful')\n",
      "(29, 'follow')\n",
      "(30, 'amazing')\n",
      "(31, 'Nadine')\n",
      "(32, 'become')\n",
      "(33, 'New')\n",
      "(34, 'soon')\n",
      "(35, 'Thanks')\n",
      "(36, 'you\\\\')\n",
      "(37, 'forever')\n",
      "(38, 'proud')\n",
      "(39, '\\xf0\\x9f\\x98\\x8d')\n",
      "(40, 'loved')\n",
      "(41, 'Always')\n",
      "(42, 'happy')\n",
      "(43, '!!')\n",
      "(44, 'sounds')\n",
      "(45, 'feels')\n",
      "(46, 'deserve')\n",
      "(47, 'support')\n",
      "(48, 'together')\n",
      "(49, 'movie')\n",
      "(50, 'Have')\n"
     ]
    }
   ],
   "source": [
    "# Write code to print words here\n",
    "\n",
    "# Write code to print words here\n",
    "from collections import defaultdict\n",
    "import math \n",
    "import operator\n",
    "\n",
    "posword_dic= defaultdict(float)\n",
    "negword_dic= defaultdict(float)\n",
    "totalword_dic= defaultdict(float)\n",
    "pos_seed=[\"good\", \"nice\", \"love\", \"excellent\", \"fortunate\", \"correct\", \"superior\"]\n",
    "neg_seed=[\"bad\", \"nasty\", \"poor\", \"hate\", \"unfortunate\", \"wrong\", \"inferior\"]\n",
    "dic_PMI = defaultdict(float)\n",
    "\n",
    "def match_2_lists(list1, list2):\n",
    "    for i in list1:\n",
    "        if i in list2:\n",
    "            return 1 \n",
    "    return 0 \n",
    "\n",
    "def cal_counts():\n",
    "    a= file (\"/Users/shivangisingh/Downloads/hw3/tweets.txt\",'r')\n",
    "    # This is doing the counts for the whole file\n",
    "    for sentence in a:\n",
    "        x=sentence.split() # x is a list of tokenized words\n",
    "        if match_2_lists(x, pos_seed):\n",
    "            for i in x:\n",
    "                if (i[0]!='.' and i[0]!='#'and i[0]!='@'and (i not in pos_seed) and (i not in neg_seed)):\n",
    "                    posword_dic[i] += 1 \n",
    "        \n",
    "        if match_2_lists(x, neg_seed):\n",
    "            for y in x:\n",
    "                if (y[0]!='.' and y[0]!='#' and y[0]!='@' and (y not in pos_seed) and (y not in neg_seed)):\n",
    "                    negword_dic[y]+=1\n",
    "                    \n",
    "#             iterate over x and add it to the positive dictionary\n",
    "        for y in x:\n",
    "            if (y[0]!='.' and y[0]!='#'and y[0]!='@'and (y not in pos_seed) and (y not in neg_seed)):\n",
    "                totalword_dic[y]+=1\n",
    "    #print posword_dic.keys()\n",
    "    \n",
    "def calculate_PMI():\n",
    "    pos = sum(posword_dic.values())\n",
    "    neg = sum(posword_dic.values())\n",
    "    for k in totalword_dic:            \n",
    "#     At this stage you have the various counts and you have to find the PMI for the two words.\n",
    "        if(totalword_dic[k]>500):\n",
    "            pos_PMI=math.log(float(posword_dic[k])+1/float((totalword_dic[k]+1)*pos))\n",
    "            neg_PMI=math.log(float(negword_dic[k])+1/float((totalword_dic[k]+1)*neg))\n",
    "            polarity_w= pos_PMI-neg_PMI\n",
    "            dic_PMI[k]=polarity_w\n",
    "    #print dic_PMI\n",
    "\n",
    "\n",
    "                         \n",
    "#         if w1 in x and w2 in x:\n",
    "#             wordtog_count+=1\n",
    "#         if w1 in x:\n",
    "#             w1_count+=1\n",
    "#         if w2 in x:\n",
    "#             w2_count+=1\n",
    "#     pmi= math.log((float(wordtog_count+1))/(float(w1_count*w2_count+1)))\n",
    "#     return pmi\n",
    "\n",
    "cal_counts()\n",
    "#print posword_dic#, negword_dic, totalword_dic\n",
    "#'''\n",
    "calculate_PMI()\n",
    "sorted_PMI = sorted(dic_PMI.items(), key=operator.itemgetter(1))\n",
    "print(\"50 NEG WORDS Filtered on count >500: \")\n",
    "for i in range(0,50):\n",
    "    print (i,sorted_PMI[i][0])\n",
    "# print(sorted_PMI[0:50])\n",
    "print\n",
    "print(\"50 POS WORDS Filtered on count >500: \")\n",
    "for i in range(1,51):\n",
    "    print (i,sorted_PMI[-i][0])\n",
    "# print(sorted_PMI[-50:])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose some of the words from both the sets of 50 words you got above which accoording to you make sense. Again please note, you will find many words which don't make sense. Do you think these results are better than the results you got in Question-1? Explain why.\n",
    "\n",
    "I think that filtering on count gave us better result as we can see more words in each category that I would classify as a word from that category like support, happy in positive and condemn and crime in the negative list. \n",
    "This happened because we removed the words whose counts were less than 500. Reducing the bias that our algorithm has towards infrequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (5 points)\n",
    "\n",
    "Even after filtering out words with count < 500, many top-most and bottom-most polarity don't make sense. Identify what kind of words these are and what can be done to filter them out. You can read some tweets in the file to see what's happening. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Textual answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that dont make sense:\n",
    "    In Negative List: Me, stands, too, says, asks, must.\n",
    "    In Positive List: have, soon,!!,become.\n",
    "\n",
    "We can filter out on prepositions, and punctuations and helping verbs also filtering emoticons would be a good idea, because people use different numbers and sequence of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, where $i$ indexes over the context types, cosine similarity is defined as follows. $x$ and $y$ are both vectors of context counts (each for a different word), where $x_i$ is the count of context $i$.\n",
    "\n",
    "$$cossim(x,y) = \\frac{ \\sum_i x_i y_i }{ \\sqrt{\\sum_i x_i^2} \\sqrt{\\sum_i y_i^2} }$$\n",
    "\n",
    "The nice thing about cosine similarity is that it is normalized: no matter what the input vectors are, the output is between 0 and 1. One way to think of this is that cosine similarity is just, um, the cosine function, which has this property (for non-negative $x$ and $y$). Another way to think of it is, to work through the situations of maximum and minimum similarity between two context vectors, starting from the definition above.\n",
    "\n",
    "Note: a good way to understand the cosine similarity function is that the numerator cares about whether the $x$ and $y$ vectors are correlated. If $x$ and $y$ tend to have high values for the same contexts, the numerator tends to be big. The denominator can be thought of as a normalization factor: if all the values of $x$ are really large, for example, dividing by the square root of their sum-of-squares prevents the whole thing from getting arbitrarily large. In fact, dividing by both these things (aka their norms) means the whole thing can’t go higher than 1.\n",
    "\n",
    "In this problem we'll work with vectors of raw context counts.  (As you know, this is not necessarily the best representation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (5 points)\n",
    "\n",
    "See the file `nytcounts.university_cat_dog`, which contains context count vectors for three words: “dog”, “cat”, and “university”. These are immediate left and right contexts from a New York Times corpus. You can open the file in a text editor since it’s quite small. \n",
    "\n",
    "Write a function which takes context count dictionaries of two words and calculates cosine similarity between these two words. The function should return a number beween 0 and 1.  Briefly comment on whether the relative simlarities make sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file nytcounts.university_cat_dog has contexts for 3 words\n",
      "['university', 'dog', 'cat']\n",
      "('dog', 'cat', 0.966882021316695)\n",
      "('cat', 'university', 0.6604270290761797)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('dog', 'university', 0.6592286372525192)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import distsim; reload(distsim)\n",
    "\n",
    "word_to_ccdict = distsim.load_contexts(\"nytcounts.university_cat_dog\")\n",
    "print(word_to_ccdict.keys())\n",
    "num=1\n",
    "\n",
    "def num_cal(a,b):\n",
    "    num = 1\n",
    "    for k, v in word_to_ccdict[a].items():\n",
    "        #pri \n",
    "        if k in word_to_ccdict[b]:\n",
    "            num+=v*word_to_ccdict[b][k]\n",
    "    den_a=den_cal(a)\n",
    "    den_b=den_cal(b)\n",
    "    \n",
    "    return a, b, (float(num)/(float((den_a)**(0.5))*(float((den_b)**(0.5)))))\n",
    "\n",
    "def den_cal(a):\n",
    "    d=1\n",
    "    for k,v in word_to_ccdict[a].items():\n",
    "        d += v**2\n",
    "    return d \n",
    "\n",
    "a=['dog','cat','university']\n",
    "for i in range(0,2):\n",
    "    print  num_cal(a[i], a[i+1])\n",
    "num_cal(a[0],a[2])\n",
    "# num_cal('dog', 'cat')\n",
    "# num_cal('dog', 'university')\n",
    "    \n",
    "    \n",
    "#print num_cal(a[i],a[i+1])\n",
    "#    print\n",
    "# write code here to show output (i.e. cosine similarity between these words.)\n",
    "# We encourage you to write other functions in distsim.py itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your response here:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the relative similarities between dog and cat does make sense as they can be both seen as animals, pets, domesticated. But dog/cat and university was 0.6 which is much more than I expected that could have been beacuse people might have tweets about missing their cat/dog while in the university or university posts about dogs, similar to the event we have at UMass petting dogs to relief stress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 6 (20 points)\n",
    "\n",
    "Explore similarities in `nytcounts.4k`, which contains context counts for about 4000 words in a sample of New York Times articles. The news data was lowercased and URLs were removed. The context counts are for the 2000 most common words in twitter, as well as the most common 2000 words in the New York Times. (But all context counts are from New York Times.) The context counts only contain contexts that appeared for more than one word.  The file has three tab-separate fields: the word, its count, and a JSON-encoded dictionary of its context counts.  You'll see it's just counts of the immediate left/right neighbors.\n",
    "\n",
    "Choose **six** words. For each, show the output of 20 nearest words (use cosine similarity as distance metric). Comment on whether the output makes sense. Comment on whether this approach to distributional similarity makes more or less sense for certain terms.\n",
    "Four of your words should be:\n",
    "\n",
    " * a name (for example: person, organization, or location)\n",
    " * a common noun\n",
    " * an adjective\n",
    " * a verb\n",
    "\n",
    "You may also want to try exploring further words that are returned from a most-similar list from one of these. You can think of this as traversing the similarity graph among words.\n",
    "\n",
    "*Implementation note:* \n",
    "On my laptop it takes several hundred MB of memory to load it into memory from the `load_contexts()` function. If you don’t have enough memory available, your computer will get very slow because the OS will start swapping. If you have to use a machine without that much memory available, you can instead implement in a streaming approach by using the `stream_contexts()` generator function to access the data; this lets you iterate through the data from disk, one vector at a time, without putting everything into memory. You can see its use in the loading function. (You could also alternatively use a key-value or other type of database, but that’s too much work for this assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file nytcounts.4k has contexts for 3648 words\n",
      "Closest words for april\n",
      "july\n",
      "march\n",
      "wednesday\n",
      "thursday\n",
      "tuesday\n",
      "monday\n",
      "friday\n",
      "saturday\n",
      "sunday\n",
      "youtube\n",
      "june\n",
      "2001\n",
      "iraq\n",
      "1996\n",
      "1999\n",
      "2002\n",
      "february\n",
      "january\n",
      "1994\n",
      "paris\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import distsim; reload(distsim)\n",
    "word_to_ccdict = distsim.load_contexts(\"nytcounts.4k\")\n",
    "###Provide your answer below; perhaps in another cell so you don't have to reload the data each time\n",
    "# print(word_to_ccdict.keys())\n",
    "words=['april','florida','prisoners','says','sorry','interview']\n",
    "temp_dict=defaultdict(float)\n",
    "\n",
    "def find_cos(a):\n",
    "    for k in word_to_ccdict:\n",
    "        if (a != k):\n",
    "            x,y,z= num_cal(a,k)\n",
    "            temp_dict[k]+=z #dictionary storing all the cosines\n",
    "    #sort dictionary\n",
    "    sorted_z = sorted(temp_dict.items(), key=operator.itemgetter(1))\n",
    "    print(\"Closest words for \"+a)\n",
    "    #for i in range (1-21):\n",
    "    #print sorted_z[-20:]\n",
    "    for j in sorted_z[-20:][::-1]:\n",
    "         print j[0]#, \"->\",  j[1]\n",
    "\n",
    "\n",
    "find_cos(words[0])\n",
    "print\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words for florida\n",
      "politics\n",
      "architecture\n",
      "chicago\n",
      "argentina\n",
      "tears\n",
      "newspapers\n",
      "animals\n",
      "cars\n",
      "connecticut\n",
      "australia\n",
      "dallas\n",
      "smoke\n",
      "canada\n",
      "atlanta\n",
      "philadelphia\n",
      "seattle\n",
      "france\n",
      "parliament\n",
      "cash\n",
      "films\n"
     ]
    }
   ],
   "source": [
    "find_cos(words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words for prisoners\n",
      "architecture\n",
      "politics\n",
      "animals\n",
      "detainees\n",
      "parliament\n",
      "newspapers\n",
      "cars\n",
      "silence\n",
      "violence\n",
      "mine\n",
      "drugs\n",
      "self\n",
      "trees\n",
      "smoke\n",
      "technology\n",
      "space\n",
      "films\n",
      "equipment\n",
      "mistakes\n",
      "fish\n"
     ]
    }
   ],
   "source": [
    "find_cos(words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words for says\n",
      "hungry\n",
      "here\n",
      "yes\n",
      "systems\n",
      "notes\n",
      "asks\n",
      "programs\n",
      "films\n",
      "2005\n",
      "australia\n",
      "politics\n",
      "everywhere\n",
      "2004\n",
      "crazy\n",
      "yeah\n",
      "standards\n",
      "anyway\n",
      "queens\n",
      "writes\n",
      "2001\n"
     ]
    }
   ],
   "source": [
    "find_cos(words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words for sorry\n",
      "hungry\n",
      "here\n",
      "systems\n",
      "fee\n",
      "yes\n",
      "programs\n",
      "reasons\n",
      "standards\n",
      "crazy\n",
      "notes\n",
      "fees\n",
      "everywhere\n",
      "films\n",
      "asks\n",
      "yeah\n",
      "politics\n",
      "later\n",
      "lately\n",
      "options\n",
      "today\n"
     ]
    }
   ],
   "source": [
    "find_cos(words[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words for interview\n",
      "empty\n",
      "idiot\n",
      "excellent\n",
      "hour\n",
      "online\n",
      "awkward\n",
      "march\n",
      "july\n",
      "icon\n",
      "incredible\n",
      "english\n",
      "awesome\n",
      "angel\n",
      "2001\n",
      "2003\n",
      "atlanta\n",
      "manhattan\n",
      "2002\n",
      "ebay\n",
      "sunday\n"
     ]
    }
   ],
   "source": [
    "find_cos(words[5])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Comment on whether the output makes sense. Comment on whether this approach to distributional similarity makes more or less sense for certain terms. \n",
    "\n",
    "words=['april','florida','prisoners','says','sorry','interview']\n",
    "1) april: The nearest neighbours were months like July, March and weekdays which I think is acceptable.\n",
    "\n",
    "2) florida: Is a state and it's neighbours chicago, dallas, argentina make sense as they are names of places that are also proper nouns.\n",
    "\n",
    "3) prisoners: gave detainees, drugs, violence and we could relate those words to prisoners. But words like tech and fish doesn't make sense on that list.\n",
    "\n",
    "4) says: asks, writes makes sense as they are also verbs but systems,notes, politics doesn't make sense in that list.\n",
    "\n",
    "5) sorry: is an adjective and is surrounded by words like hungry, crazy are adjectives that is fine. But many of the words on the neighbouring list dont make sense like everywhere, politics.\n",
    "\n",
    "6) interview : neighbours like awesome, awkward, incredible, online make sense as they could be used to describe the interview.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (10 points)\n",
    "\n",
    "In the next several questions, you'll examine similarities in trained word embeddings, instead of raw context counts.\n",
    "\n",
    "See the file `nyt_word2vec.university_cat_dog`, which contains word embedding vectors pretrained by word2vec [1] for three words: “dog”, “cat”, and “university”, from the same corpus. You can open the file in a text editor since it’s quite small.\n",
    "\n",
    "Write a function which takes word embedding vectors of two words and calculates cossine similarity between these 2 words. The function should return a number beween -1 and 1. Briefly comment on whether the relative simlarities make sense. \n",
    "\n",
    "*Implementation note:*\n",
    "Notice that the inputs of this function are numpy arrays (v1 and v2). If you are not very familiar with the basic operation in numpy, you can find some examples in the basic operation section here:\n",
    "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\n",
    "\n",
    "If you know how to use Matlab but haven't tried numpy before, the following link should be helpful:\n",
    "https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html\n",
    "\n",
    "[1] Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" NIPS 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import distsim; reload(distsim)\n",
    "\n",
    "word_to_vec_dict = distsim.load_word2vec(\"nyt_word2vec.4k\")\n",
    "# print(word_to_vec_dict['sleep'])\n",
    "# takes in two words and iterates over the array and finds cosine similarity\n",
    "\n",
    "def find_cosine_with_arrays_values(x,y):\n",
    "    a=word_to_vec_dict[x]\n",
    "    b=word_to_vec_dict[y]\n",
    "    nume=0\n",
    "    for i in range(0,len(a)):\n",
    "        nume+=a[i]*b[i]\n",
    "    den=normalizer_x(a)*normalizer_x(b)\n",
    "    print (x,y,float(nume/den))\n",
    "    \n",
    "\n",
    "def normalizer_x(a):\n",
    "    sum=0.0\n",
    "    for i in range(0,len(a)):\n",
    "        sum+=a[i]**2\n",
    "    return(float(sum**(0.5)))\n",
    "\n",
    "# write code here to show output (i.e. cosine similarity between these words.)\n",
    "# We encourage you to write other functions in distsim.py itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sleep', 'limited', -0.05615957527466608)\n",
      "('sleep', 'inning', 0.16911608244656046)\n",
      "('sleep', 'copy', 0.04050483447125724)\n",
      "('sleep', '9/11', 0.03563852340348753)\n",
      "('sleep', 'yellow', 0.2152959760557489)\n",
      "('sleep', 'four', 0.06165594500757751)\n",
      "('sleep', 'woods', 0.20389156413001522)\n",
      "('sleep', 'asian', -0.17966727043568964)\n",
      "('sleep', 'aides', 0.025835853483555403)\n",
      "('sleep', 'captain', 0.01617202529716166)\n",
      "('sleep', 'hate', 0.2922239121025376)\n",
      "('sleep', 'towns', 0.11121305979710545)\n",
      "('sleep', 'forget', 0.5051791644223588)\n",
      "('sleep', 'relationships', 0.14806902828986734)\n",
      "('sleep', 'whose', -0.37639421223122527)\n",
      "('sleep', 'votes', 0.06909716199571934)\n",
      "('sleep', 'founder', -0.354974289037227)\n",
      "('sleep', 'paris', 0.005012282917336253)\n",
      "('sleep', 'adviser', -0.15702685593006174)\n",
      "('sleep', 'crush', 0.39296779913238783)\n",
      "('sleep', 'edward', -0.24268931930999446)\n",
      "('sleep', 'voted', 0.010704740438751501)\n",
      "('sleep', 'under', -0.06432113599881244)\n",
      "('sleep', 'teaching', 0.14171084355353747)\n",
      "('sleep', 'sorry', 0.22942270200542267)\n",
      "('sleep', 'pride', 0.14959239667043173)\n",
      "('sleep', 'worth', 0.11238040839277275)\n",
      "('sleep', 'risk', 0.21377360482600472)\n",
      "('sleep', 'rise', 0.18132262573142535)\n",
      "('sleep', 'shoot', 0.5622848864083151)\n",
      "('sleep', 'every', 0.2193281791186506)\n",
      "('sleep', 'jack', -0.09516895898462581)\n",
      "('sleep', 'kick', 0.5129092799871635)\n",
      "('sleep', 'jacob', -0.11315883654949813)\n",
      "('sleep', 'bringing', 0.06117736700532818)\n",
      "('sleep', 'vast', -0.16569131092718856)\n",
      "('sleep', 'tickets', 0.059974042362028905)\n",
      "('sleep', 'school', 0.1090823151073531)\n",
      "('sleep', 'prize', -0.24780324477135338)\n",
      "('sleep', 'skills', 0.21116192885853247)\n",
      "('sleep', 'companies', -0.06416610600463979)\n",
      "('sleep', 'wednesday', 0.08015776855431275)\n",
      "('sleep', 'red', 0.02986302055063737)\n",
      "('sleep', 'heading', 0.07670774917950755)\n",
      "('sleep', 'confidence', 0.134838956961781)\n",
      "('sleep', 'jeff', -0.106556386812777)\n",
      "('sleep', 'force', 0.02845630634240158)\n",
      "('sleep', 'leaders', 0.013023217491122474)\n",
      "('sleep', 'tired', 0.3136355347746915)\n",
      "('sleep', 'awake', 0.5821219157083922)\n",
      "('sleep', 'miller', -0.10064471410886723)\n",
      "('sleep', 'hanging', 0.3498889082510633)\n",
      "('sleep', 'estimates', -0.16644899319804038)\n",
      "('sleep', 'japanese', -0.14181171999516404)\n",
      "('sleep', 'elections', -0.06858462652367203)\n",
      "('sleep', 'chef', -0.08388160190270291)\n",
      "('sleep', 'cameron', -0.048841290743557864)\n",
      "('sleep', 'second', 0.09003872285932783)\n",
      "('sleep', 'street', 0.02116978742169018)\n",
      "('sleep', '7th', 0.13506600097924445)\n",
      "('sleep', 'estimated', -0.14713710408191943)\n",
      "('sleep', 'machines', 0.16174363509942075)\n",
      "('sleep', 'monster', 0.081330166919357)\n",
      "('sleep', 'even', 0.2359714040898272)\n",
      "('sleep', 'established', -0.23095353776600835)\n",
      "('sleep', 'liam', 0.013908117128710455)\n",
      "('sleep', 'hide', 0.5265850224198004)\n",
      "('sleep', 'christ', 0.07608164128120214)\n",
      "('sleep', 'decisions', 0.16427771178249945)\n",
      "('sleep', '+', 0.03910463108891648)\n",
      "('sleep', 'orchestra', -0.13639439461923228)\n",
      "('sleep', 'while', 0.10500621984108349)\n",
      "('sleep', 'bro', -0.07019842087074679)\n",
      "('sleep', 'spokesman', -0.1365187995485273)\n",
      "('sleep', 'lights', 0.27713023499548584)\n",
      "('sleep', 'en', 0.06909110412873895)\n",
      "('sleep', 'above', -0.03036564287057258)\n",
      "('sleep', 'dr.', -0.0043305380041440975)\n",
      "('sleep', 'new', -0.11836292797573625)\n",
      "('sleep', 'net', 0.13304845121717845)\n",
      "('sleep', 'increasing', -0.06338444274574913)\n",
      "('sleep', 'ever', 0.09858103429660395)\n",
      "('sleep', 'disney', -0.19668868419244767)\n",
      "('sleep', 'told', 0.06376403458727675)\n",
      "('sleep', 'specialist', -0.004128784778552242)\n",
      "('sleep', 'hero', -0.015402706428803519)\n",
      "('sleep', 'reporter', -0.08267731142895003)\n",
      "('sleep', 'men', 0.21577979880819348)\n",
      "('sleep', 'here', 0.209046060084749)\n",
      "('sleep', 'hundreds', 0.13325527148065625)\n",
      "('sleep', 'reported', -0.13532698450695044)\n",
      "('sleep', 'protection', 0.09604802955110213)\n",
      "('sleep', 'china', -0.05666135948325837)\n",
      "('sleep', 'active', -0.04594527034866111)\n",
      "('sleep', 'path', 0.21504217794383532)\n",
      "('sleep', '100', 0.11678997223089885)\n",
      "('sleep', 'practices', 0.09045723274746766)\n",
      "('sleep', 'buddy', 0.16187167675872915)\n",
      "('sleep', 'dry', 0.3746106355020787)\n",
      "('sleep', 'hell', 0.37374233906074394)\n",
      "('sleep', 'voice', 0.11090493275244179)\n",
      "('sleep', 'daughter', 0.24186125731439279)\n",
      "('sleep', 'auction', -0.13388158339217932)\n",
      "('sleep', 'items', 0.15255698300032813)\n",
      "('sleep', 'study', -0.0051722133459519365)\n",
      "('sleep', 'changed', 0.0781867097369281)\n",
      "('sleep', 'jail', 0.3435315640510637)\n",
      "('sleep', 'reports', -0.026099906981787366)\n",
      "('sleep', 'robin', -0.05241672317152465)\n",
      "('sleep', 'itunes', 0.028007018287231497)\n",
      "('sleep', 'credit', 0.12770310565630213)\n",
      "('sleep', 'analysts', -0.19759496863831302)\n",
      "('sleep', 'smoke', 0.4823473166012664)\n",
      "('sleep', 'military', 0.03825645340654958)\n",
      "('sleep', 'aka', -0.07868995896297454)\n",
      "('sleep', 'changes', 0.05452580423856422)\n",
      "('sleep', 'tweet', 0.28142848596250997)\n",
      "('sleep', 'criticism', -0.042057558395889524)\n",
      "('sleep', 'golden', -0.10738289851630999)\n",
      "('sleep', 'fantastic', 0.048980110949175745)\n",
      "('sleep', 'campaign', -0.12415476872101308)\n",
      "('sleep', 'explained', 0.05904145871129663)\n",
      "('sleep', 'highly', -0.16626689944491074)\n",
      "('sleep', 'county', -0.054510015632548134)\n",
      "('sleep', 'moral', -0.06893578876599116)\n",
      "('sleep', 'guests', 0.3545468540606245)\n",
      "('sleep', 'total', -0.06233514493644582)\n",
      "('sleep', 'unit', -0.08250380077590828)\n",
      "('sleep', 'opponents', 0.058050329456362876)\n",
      "('sleep', 'spoke', 0.20402956492942143)\n",
      "('sleep', 'would', 0.007307382226866265)\n",
      "('sleep', 'army', 0.07033281157663326)\n",
      "('sleep', 'hospital', 0.2811818243770051)\n",
      "('sleep', 'm.', -0.23897735534884867)\n",
      "('sleep', 'arms', 0.303202852139722)\n",
      "('sleep', 'symphony', -0.18378507894877807)\n",
      "('sleep', 'music', -0.011310340729251413)\n",
      "('sleep', 'calm', 0.2967287218420123)\n",
      "('sleep', 'strike', 0.3193434332339579)\n",
      "('sleep', 'sexy', 0.17384179368641323)\n",
      "('sleep', 'preview', -0.01113376571730462)\n",
      "('sleep', 'type', 0.11137336640967212)\n",
      "('sleep', 'until', 0.11904440911325212)\n",
      "('sleep', '1000', -0.14024723119480176)\n",
      "('sleep', 'breathe', 0.649710870847066)\n",
      "('sleep', 'females', 0.29718133139383957)\n",
      "('sleep', 'supporters', -0.0262230322451819)\n",
      "('sleep', 'holy', 0.031610403215251846)\n",
      "('sleep', 'successful', -0.18355025565585453)\n",
      "('sleep', 'brings', 0.017762924153969045)\n",
      "('sleep', 'award', -0.31601049135347914)\n",
      "('sleep', 'hurt', 0.3449596386698621)\n",
      "('sleep', 'phone', 0.2461779735621586)\n",
      "('sleep', 'connecticut', -0.07449724015721969)\n",
      "('sleep', 'warm', 0.33560877289210794)\n",
      "('sleep', 'adult', 0.17119415266927787)\n",
      "('sleep', 'excellent', -0.09309029032510364)\n",
      "('sleep', '90', 0.14272229315380852)\n",
      "('sleep', 'hole', 0.1687271085424737)\n",
      "('sleep', 'hold', 0.4188555831471237)\n",
      "('sleep', 'squad', 0.050662262992632066)\n",
      "('sleep', 'must', -0.08976927437786686)\n",
      "('sleep', 'me', 0.44215903540616175)\n",
      "('sleep', '1990', -0.021956498169130523)\n",
      "('sleep', 'te', -0.03004137717525539)\n",
      "('sleep', 'word', 0.14304376842979608)\n",
      "('sleep', 'room', 0.2803253730459774)\n",
      "('sleep', '1997', -0.15080778295420916)\n",
      "('sleep', 'rights', -0.003564144827340539)\n",
      "('sleep', '1999', -0.13416062109253832)\n",
      "('sleep', '1998', -0.17642176573852184)\n",
      "('sleep', 'mo', 0.04297865646459365)\n",
      "('sleep', 'work', 0.4148218396092897)\n",
      "('sleep', 'mi', -0.06391715336377997)\n",
      "('sleep', 'roof', 0.13462379214712422)\n",
      "('sleep', 'movies', 0.11001288855599617)\n",
      "('sleep', 'cuddle', 0.19690213446964952)\n",
      "('sleep', 'era', -0.11335531469369138)\n",
      "('sleep', 'henry', -0.25104413987665863)\n",
      "('sleep', 'im', -0.05522704771795444)\n",
      "('sleep', 'my', 0.310069737082251)\n",
      "('sleep', 'example', -0.06208630276422272)\n",
      "('sleep', 'boyfriend', 0.3221801644093774)\n",
      "('sleep', 'estate', -0.1666818346453118)\n",
      "('sleep', 'give', 0.2872074624451422)\n",
      "('sleep', 'cited', -0.21491844931419635)\n",
      "('sleep', 'india', -0.1288825771747457)\n",
      "('sleep', 'honey', 0.3236057995540459)\n",
      "('sleep', 'calif.', -0.07172096667827721)\n",
      "('sleep', 'want', 0.3113191080886121)\n",
      "('sleep', 'david', -0.1893704759101361)\n",
      "('sleep', 'worse', 0.29575234080833396)\n",
      "('sleep', 'ceremony', 0.07033555917608657)\n",
      "('sleep', 'admission', -0.030685544989504127)\n",
      "('sleep', 'end', 0.1848027911879775)\n",
      "('sleep', 'thing', 0.18482435684409446)\n",
      "('sleep', 'provide', 0.2694087927873454)\n",
      "('sleep', 'travel', 0.28134660700072717)\n",
      "('sleep', 'sitting', 0.3560159694951854)\n",
      "('sleep', 'damage', 0.28277362962429087)\n",
      "('sleep', 'id', 0.03661651272694796)\n",
      "('sleep', 'machine', 0.15509911035919255)\n",
      "('sleep', 'how', 0.14134821835961744)\n",
      "('sleep', 'hot', 0.31516813907537256)\n",
      "('sleep', 'writers', -0.008175362939928868)\n",
      "('sleep', 'hop', 0.3794728408042362)\n",
      "('sleep', 'interview', 0.08051690084894482)\n",
      "('sleep', 'beach', 0.15400989427606407)\n",
      "('sleep', 'gordon', -0.19777241072750362)\n",
      "('sleep', 'grown', 0.05376899890128714)\n",
      "('sleep', 'regional', -0.2840042791615036)\n",
      "('sleep', 'elizabeth', -0.08696100219801686)\n",
      "('sleep', 'beauty', 0.07182544660633232)\n",
      "('sleep', 'mess', 0.32730738419054706)\n",
      "('sleep', 'after', 0.05757621870210022)\n",
      "('sleep', 'confirmed', -0.1063074795027062)\n",
      "('sleep', 'wrong', 0.29784051847144033)\n",
      "('sleep', 'lay', 0.3733584159682308)\n",
      "('sleep', 'president', -0.24292072679251486)\n",
      "('sleep', 'law', -0.021339060510312258)\n",
      "('sleep', 'excited', 0.12276116857344715)\n",
      "('sleep', 'las', 0.030122741426274788)\n",
      "('sleep', 'purchase', 0.07349039821721998)\n",
      "('sleep', 'attempt', 0.015510456893580908)\n",
      "('sleep', 'third', 0.05370980379089706)\n",
      "('sleep', 'wins', -0.02143982906736556)\n",
      "('sleep', 'acknowledged', -0.1265258104995764)\n",
      "('sleep', 'headquarters', -0.06707049831572276)\n",
      "('sleep', 'childhood', 0.25501624113261767)\n",
      "('sleep', 'maintain', 0.21707849373307989)\n",
      "('sleep', 'green', 0.10648713304369949)\n",
      "('sleep', 'ultimate', -0.17078804068311115)\n",
      "('sleep', 'goodbye', 0.35584977209734414)\n",
      "('sleep', 'suggest', 0.17677239760866012)\n",
      "('sleep', 'button', 0.256351372646392)\n",
      "('sleep', 'fan', -0.022187565605932883)\n",
      "('sleep', 'democratic', -0.17932541951357595)\n",
      "('sleep', 'order', 0.14495919531315068)\n",
      "('sleep', 'wind', 0.19906360120014988)\n",
      "('sleep', 'wine', 0.0652170089910754)\n",
      "('sleep', 'operations', 0.02776090767577777)\n",
      "('sleep', 'senators', -0.0488435044612323)\n",
      "('sleep', 'office', 0.052246572028123196)\n",
      "('sleep', 'enter', 0.3382596122117161)\n",
      "('sleep', 'over', 0.012374893662454693)\n",
      "('sleep', 'teen', 0.1416732452593395)\n",
      "('sleep', 'fab', 0.2805161000473042)\n",
      "('sleep', 'london', -0.17737255452518913)\n",
      "('sleep', 'japan', -0.17233988772322292)\n",
      "('sleep', 'nets', 0.12612851172924475)\n",
      "('sleep', 'mayor', -0.1595351074849978)\n",
      "('sleep', 'before', 0.16296485757885687)\n",
      "('sleep', 'fit', 0.41084909107292844)\n",
      "('sleep', 'personal', 0.0641920327372485)\n",
      "('sleep', 'del', -0.21171271293610244)\n",
      "('sleep', 'fix', 0.41954020418034893)\n",
      "('sleep', ',', 0.0205875213956381)\n",
      "('sleep', 'writing', 0.09352199512683457)\n",
      "('sleep', 'better', 0.21226477123316165)\n",
      "('sleep', 'production', -0.19828410577500866)\n",
      "('sleep', '400', 0.028516040009238363)\n",
      "('sleep', 'weeks', 0.12105970553662554)\n",
      "('sleep', 'easier', 0.11416533293017703)\n",
      "('sleep', 'freak', 0.3215749079614491)\n",
      "('sleep', 'then', 0.21936123259612586)\n",
      "('sleep', 'them', 0.4052556148281122)\n",
      "('sleep', 'workout', 0.4262819183315778)\n",
      "('sleep', 'safe', 0.22949711446335766)\n",
      "('sleep', 'break', 0.44307174754470335)\n",
      "('sleep', 'band', -0.0030490459042645138)\n",
      "('sleep', 'bang', 0.31438435878759147)\n",
      "('sleep', 'heaven', 0.19062388093705526)\n",
      "('sleep', 'effects', 0.15519820729085104)\n",
      "('sleep', 'they', 0.244418136525251)\n",
      "('sleep', 'schools', 0.14730321335020952)\n",
      "('sleep', 'yourself', 0.30140780516660853)\n",
      "('sleep', 'silver', -0.12161105759002636)\n",
      "('sleep', 'bank', -0.13456082212550707)\n",
      "('sleep', 'represents', -0.2839303321953003)\n",
      "('sleep', 'ruth', -0.026457111095248734)\n",
      "('sleep', 'alex', 0.02533086266769834)\n",
      "('sleep', 'debut', -0.0556753845451464)\n",
      "('sleep', 'arrested', 0.129611912474918)\n",
      "('sleep', 'l', 0.07369268852695356)\n",
      "('sleep', 'detroit', -0.0476443314982718)\n",
      "('sleep', 'victory', 0.020291783530453966)\n",
      "('sleep', 'each', 0.15301490189627007)\n",
      "('sleep', 'went', 0.22322814224320042)\n",
      "('sleep', 'side', 0.1413764509582146)\n",
      "('sleep', 'mean', 0.3836597769792383)\n",
      "('sleep', 'signing', 0.04026173279084777)\n",
      "('sleep', 'financial', -0.14519540932945116)\n",
      "('sleep', 'series', -0.05769498096975969)\n",
      "('sleep', 'crimes', 0.16008321992863683)\n",
      "('sleep', 'solution', 0.051063897470120974)\n",
      "('sleep', 'carry', 0.4092371029132067)\n",
      "('sleep', 'bra', 0.12378201499679384)\n",
      "('sleep', 'taught', 0.1196545806652102)\n",
      "('sleep', 'trading', -0.060420092130240875)\n",
      "('sleep', 'forgot', 0.3330834101370823)\n",
      "('sleep', 'saturday', 0.18450213188283657)\n",
      "('sleep', 'rt', 0.026876518090135644)\n",
      "('sleep', 't.', -0.2782340898264365)\n",
      "('sleep', 'gon', 0.2502146851802974)\n",
      "('sleep', 'network', -0.027817906492107955)\n",
      "('sleep', 'driving', 0.2926125096109721)\n",
      "('sleep', 'god', 0.3112867064773482)\n",
      "('sleep', 'crucial', -0.2558751277652579)\n",
      "('sleep', 'tomorrow', 0.1552246058573958)\n",
      "('sleep', 'content', 0.1292822276726448)\n",
      "('sleep', 're', -0.002918238553492845)\n",
      "('sleep', 'daniel', -0.2407951828520915)\n",
      "('sleep', 'medicine', 0.14992285559683646)\n",
      "('sleep', 'got', 0.2995607327003839)\n",
      "('sleep', 'millions', 0.07896487447627781)\n",
      "('sleep', 'foundation', -0.22116127038797656)\n",
      "('sleep', 'sept.', 0.01209601205796873)\n",
      "('sleep', 'turning', 0.10401618438924957)\n",
      "('sleep', 'cutest', 0.17648441065346188)\n",
      "('sleep', 'associate', -0.14950842150286947)\n",
      "('sleep', 'u.s.', -0.05757136827630922)\n",
      "('sleep', 'fave', -0.03641430498231449)\n",
      "('sleep', 'little', 0.22259212819996663)\n",
      "('sleep', 'free', 0.09332335515110682)\n",
      "('sleep', 'standard', -0.04183064726904283)\n",
      "('sleep', 'struggle', 0.10850506315850576)\n",
      "('sleep', 'wanted', 0.08750648759144994)\n",
      "('sleep', 'storm', 0.1398164763216393)\n",
      "('sleep', 'ate', 0.4551402767421171)\n",
      "('sleep', 'created', -0.27237536361790654)\n",
      "('sleep', 'starts', 0.208751049174996)\n",
      "('sleep', 'makeup', 0.26536144318246324)\n",
      "('sleep', 'messages', 0.20597815948506465)\n",
      "('sleep', 'days', 0.19745354129200568)\n",
      "('sleep', 'musicians', -0.009495420689586006)\n",
      "('sleep', 'tied', 0.05639091077255085)\n",
      "('sleep', 'moving', 0.09570826732865442)\n",
      "('sleep', 'r.', -0.3666485392913675)\n",
      "('sleep', 'clothes', 0.4471049390551341)\n",
      "('sleep', 'onto', 0.13257528269975907)\n",
      "('sleep', 'bite', 0.5015701832534488)\n",
      "('sleep', 'stewart', -0.022242371578722143)\n",
      "('sleep', 'enjoy', 0.5064435673858789)\n",
      "('sleep', 'already', -0.10823729238362534)\n",
      "('sleep', 'characters', 0.10276596326744779)\n",
      "('sleep', 'grade', 0.0754824892040243)\n",
      "('sleep', 'primary', -0.11081892528683981)\n",
      "('sleep', 'hearing', 0.1675686412209461)\n",
      "('sleep', 'floors', 0.1264612925683802)\n",
      "('sleep', 'another', 0.011457793446078869)\n",
      "('sleep', 'thick', 0.26304797843485694)\n",
      "('sleep', 'electronic', -0.0694801104536173)\n",
      "('sleep', 'hood', 0.11937664255582903)\n",
      "('sleep', 'basketball', 0.02135568366289384)\n",
      "('sleep', 'service', 0.08341152223512265)\n",
      "('sleep', 'top', -0.07033375713683838)\n",
      "('sleep', 'girls', 0.3299521537108134)\n",
      "('sleep', 'returns', 0.13515407646564423)\n",
      "('sleep', 'needed', 0.13630294004830468)\n",
      "('sleep', 'rates', 0.10746645220360176)\n",
      "('sleep', 'too', 0.2667287635038695)\n",
      "('sleep', 'tom', -0.19052393867403544)\n",
      "('sleep', 'architect', -0.3701554176671037)\n",
      "('sleep', 'hollywood', -0.17336738025874476)\n",
      "('sleep', 'john', -0.18394088674689923)\n",
      "('sleep', 'dogs', 0.5173440573974232)\n",
      "('sleep', 'listen', 0.4655233472002008)\n",
      "('sleep', 'urban', -0.15846702299246232)\n",
      "('sleep', 'murder', 0.1172363452756154)\n",
      "('sleep', 'grandchildren', 0.1953484256970722)\n",
      "('sleep', 'bacon', 0.22119073582098042)\n",
      "('sleep', 'gym', 0.4534076762468757)\n",
      "('sleep', 'serve', 0.3901130950343433)\n",
      "('sleep', 'took', 0.1579692428620957)\n",
      "('sleep', 'madison', 0.014327621113218633)\n",
      "('sleep', 'direct', 0.011847576815282916)\n",
      "('sleep', 'western', -0.17505805673113797)\n",
      "('sleep', 'somewhat', -0.0819484647790476)\n",
      "('sleep', 'evil', 0.1748050692989078)\n",
      "('sleep', 'shortly', 0.09750346770597676)\n",
      "('sleep', 'artistic', -0.2397625896262931)\n",
      "('sleep', 'begins', 0.09066262894043622)\n",
      "('sleep', 'distance', 0.3009894863016129)\n",
      "('sleep', 'target', -0.036762573500508766)\n",
      "('sleep', 'showed', -0.019066214232934576)\n",
      "('sleep', 'princeton', -0.1510420682251234)\n",
      "('sleep', 'immigrants', 0.15060953438944447)\n",
      "('sleep', 'tree', 0.2604704605943616)\n",
      "('sleep', 'likely', -0.03504716507527141)\n",
      "('sleep', 'nations', -0.0450029753187019)\n",
      "('sleep', 'project', -0.1908105309296447)\n",
      "('sleep', 'matter', 0.18109946435084737)\n",
      "('sleep', 'classes', 0.21404162136347862)\n",
      "('sleep', 'silly', 0.14024891953363763)\n",
      "('sleep', 'friend', 0.17756914276165336)\n",
      "('sleep', 'historical', -0.22679568155873117)\n",
      "('sleep', 'k', -0.0374804143216555)\n",
      "('sleep', 'feeling', 0.31228430253575573)\n",
      "('sleep', 'brilliant', -0.1285727500665579)\n",
      "('sleep', 'bridge', -0.0153533487269338)\n",
      "('sleep', 'fashion', -0.07309936514197932)\n",
      "('sleep', 'sees', 0.05032406559554467)\n",
      "('sleep', 'ran', 0.07710506695289905)\n",
      "('sleep', 'boston', -0.05590606962176337)\n",
      "('sleep', 'modern', -0.18135545279196852)\n",
      "('sleep', 'mind', 0.3313510452992726)\n",
      "('sleep', 'mine', 0.1671833703256748)\n",
      "('sleep', 'anybody', 0.35058417757142996)\n",
      "('sleep', 'talking', 0.2841031338718292)\n",
      "('sleep', 'rap', 0.008728339299228288)\n",
      "('sleep', 'seen', 0.11652999601357716)\n",
      "('sleep', 'seem', 0.2437008900220084)\n",
      "('sleep', 'ocean', 0.02333930240670402)\n",
      "('sleep', 'seek', 0.24820383754459258)\n",
      "('sleep', 'tells', 0.06495975510284233)\n",
      "('sleep', 'alive', 0.2638033221583126)\n",
      "('sleep', 'dozen', 0.07341568619453911)\n",
      "('sleep', 'forced', 0.0045802751307529276)\n",
      "('sleep', 'strength', 0.03524764323819962)\n",
      "('sleep', \"''but\", 0.25345674028389004)\n",
      "('sleep', 'responsible', 0.0097128370989373)\n",
      "('sleep', '1960', -0.06438444798812137)\n",
      "('sleep', 'snow', 0.19487573529830068)\n",
      "('sleep', 'contact', 0.3618367175421838)\n",
      "('sleep', \"''you\", 0.15585575912330277)\n",
      "('sleep', 'laughed', 0.3731897918436121)\n",
      "('sleep', 'blue', 0.007617642339799265)\n",
      "('sleep', 'nobody', 0.22492751543880812)\n",
      "('sleep', 'though', 0.059534734276256634)\n",
      "('sleep', 'bruce', -0.2070591554193195)\n",
      "('sleep', 'doin', 0.12087601614114965)\n",
      "('sleep', 'y', -0.029123220273992)\n",
      "('sleep', 'regular', 0.15292436209779395)\n",
      "('sleep', 'germany', -0.1724068451831759)\n",
      "('sleep', 'letter', -0.05025239883417701)\n",
      "('sleep', 'hearts', 0.34261545249906344)\n",
      "('sleep', 'laughing', 0.4506619083216413)\n",
      "('sleep', 'singer', -0.11860758241352598)\n",
      "('sleep', 'stupid', 0.28834582812163145)\n",
      "('sleep', 'episode', 0.021702411606279063)\n",
      "('sleep', 'consumers', 0.0951575584601077)\n",
      "('sleep', 'professor', -0.15460730074116552)\n",
      "('sleep', 'camp', 0.29679247466147174)\n",
      "('sleep', 'metal', 0.11495259773834425)\n",
      "('sleep', 'dog', 0.42257834598422866)\n",
      "('sleep', 'yours', 0.36405386170830256)\n",
      "('sleep', 'points', 0.009378282053002751)\n",
      "('sleep', 'tech', -0.10581902174289923)\n",
      "('sleep', 'deeply', 0.021857776554988245)\n",
      "('sleep', 'voting', 0.00873734805212599)\n",
      "('sleep', 'consumer', -0.15491754798211485)\n",
      "('sleep', 'responded', 0.06430542141486748)\n",
      "('sleep', 'came', 0.13480710409453303)\n",
      "('sleep', 'reserve', -0.05683785826616683)\n",
      "('sleep', 'd.', -0.28043867404969014)\n",
      "('sleep', 'saying', 0.13230830825480086)\n",
      "('sleep', 'bomb', 0.20496850949820924)\n",
      "('sleep', 'lyrics', 0.058491378443450766)\n",
      "('sleep', 'meetings', 0.1228877137641041)\n",
      "('sleep', 'queen', 0.06407120937183745)\n",
      "('sleep', 'ending', -0.006559116254180165)\n",
      "('sleep', 'qaeda', 0.00767979788597995)\n",
      "('sleep', 'radio', -0.08187925850229662)\n",
      "('sleep', 'earth', 0.23591212866853944)\n",
      "('sleep', 'enjoyed', 0.057647275778540674)\n",
      "('sleep', 'pick', 0.28295556901882074)\n",
      "('sleep', 'orleans', 0.02968918206347504)\n",
      "('sleep', 'judges', 0.09648269580011194)\n",
      "('sleep', 'busy', 0.2126670972767412)\n",
      "('sleep', 'just', 0.33550509316314986)\n",
      "('sleep', 'menu', 0.1470782957313029)\n",
      "('sleep', 'explain', 0.24624958503209526)\n",
      "('sleep', 'sugar', 0.2619125051574397)\n",
      "('sleep', 'rihanna', 0.015586999628049991)\n",
      "('sleep', 'bush', 0.09275487620157148)\n",
      "('sleep', 'rich', 0.004489176253632501)\n",
      "('sleep', 'folks', 0.32836040715410353)\n",
      "('sleep', 'rice', 0.13335948257542174)\n",
      "('sleep', 'personality', 0.06595114405293095)\n",
      "('sleep', 'wearing', 0.25068572175265547)\n",
      "('sleep', 'do', 0.4377268264853003)\n",
      "('sleep', 'dj', -0.03735107662230099)\n",
      "('sleep', 'wide', -0.07349310493930318)\n",
      "('sleep', 'de', -0.15248337449312324)\n",
      "('sleep', 'watch', 0.49267390689822765)\n",
      "('sleep', '13', 0.10518086836841413)\n",
      "('sleep', 'legs', 0.49640388189993956)\n",
      "('sleep', 'coast', -0.10501990309846367)\n",
      "('sleep', 'criticized', -0.13889766434854064)\n",
      "('sleep', 'amazon', -0.06869127063094684)\n",
      "('sleep', 'christopher', -0.23221551116116215)\n",
      "('sleep', 'despite', -0.11040124443646694)\n",
      "('sleep', 'report', 0.040670399569133035)\n",
      "('sleep', 'ben', -0.11541306286870118)\n",
      "('sleep', 'nasty', 0.22802240294658174)\n",
      "('sleep', 'dr', 0.030951464735622974)\n",
      "('sleep', 'tips', 0.3342902364539549)\n",
      "('sleep', 'runs', 0.07136316817116575)\n",
      "('sleep', \"''no\", 0.132504911933411)\n",
      "('sleep', 'bar', 0.19431859191483405)\n",
      "('sleep', 'countries', 0.06463082149798152)\n",
      "('sleep', 'fields', 0.020430336604308742)\n",
      "('sleep', 'hello', 0.20326700738682668)\n",
      "('sleep', 'bay', 0.08158803373115037)\n",
      "('sleep', 'twice', 0.19985635203208582)\n",
      "('sleep', 'bad', 0.3030499865511193)\n",
      "('sleep', 'bae', -0.014440708618239784)\n",
      "('sleep', 'architecture', -0.17713312961251176)\n",
      "('sleep', 'shots', 0.2662297426813459)\n",
      "('sleep', 'rival', -0.20964363626912888)\n",
      "('sleep', 'release', 0.12423795779519244)\n",
      "('sleep', 'steal', 0.4537289497318434)\n",
      "('sleep', 'secretary', -0.07242470625765275)\n",
      "('sleep', 'headed', 0.005677452547264765)\n",
      "('sleep', 'fair', -0.11342982901009227)\n",
      "('sleep', 'w.', -0.27548693759729265)\n",
      "('sleep', 'testing', 0.1240395726178305)\n",
      "('sleep', 'goodnight', -0.1589634211342495)\n",
      "('sleep', 'decided', 0.04724892371168795)\n",
      "('sleep', 'result', 0.026383435221966792)\n",
      "('sleep', 'fail', 0.3607098499513033)\n",
      "('sleep', 'christian', -0.1666170093008523)\n",
      "('sleep', 'activities', 0.23948599327501968)\n",
      "('sleep', 'best', 0.03959187615650164)\n",
      "('sleep', 'subject', -0.05095424457481874)\n",
      "('sleep', 'brazil', -0.1828471262519088)\n",
      "('sleep', 'said', 0.013332667141736865)\n",
      "('sleep', 'lots', 0.22993863412449211)\n",
      "('sleep', 'away', 0.29654202715777644)\n",
      "('sleep', 'prince', -0.0889000325153313)\n",
      "('sleep', 'grandmother', 0.28017730356803516)\n",
      "('sleep', 'future', 0.08278732765865193)\n",
      "('sleep', 'finger', 0.36203830237799156)\n",
      "('sleep', 'hopefully', 0.349238357207895)\n",
      "('sleep', 'mum', 0.04818762855604724)\n",
      "('sleep', 'drawn', -0.08880822206344463)\n",
      "('sleep', 'approach', 0.0929834494329317)\n",
      "('sleep', 'we', 0.2418667008226611)\n",
      "('sleep', 'never', 0.1593017464661161)\n",
      "('sleep', 'terms', 0.019299140396587595)\n",
      "('sleep', 'extend', 0.25425103268771554)\n",
      "('sleep', 'nature', 0.1090289373964981)\n",
      "('sleep', 'wo', 0.1158652732969914)\n",
      "('sleep', 'literally', 0.3343480841617824)\n",
      "('sleep', 'weak', -0.034458945002613324)\n",
      "('sleep', 'however', -0.016212868670896496)\n",
      "('sleep', 'boss', 0.21882315621469248)\n",
      "('sleep', 'halloween', 0.2371667945230235)\n",
      "('sleep', 'harvard', -0.09035670017634005)\n",
      "('sleep', 'drew', -0.12955867098364765)\n",
      "('sleep', 'news', -0.0016835934484173737)\n",
      "('sleep', 'picking', 0.20442270916926103)\n",
      "('sleep', 'debt', 0.003314922557758257)\n",
      "('sleep', 'kitchen', 0.31278284351499286)\n",
      "('sleep', 'suggested', -0.07700701476744337)\n",
      "('sleep', 'received', -0.02337161956136797)\n",
      "('sleep', 'protect', 0.33381433538152383)\n",
      "('sleep', 'accident', 0.1790014164227329)\n",
      "('sleep', 'met', 0.1399492661862155)\n",
      "('sleep', 'country', 0.09785923309819144)\n",
      "('sleep', 'ill', 0.41121792723947537)\n",
      "('sleep', 'readers', 0.17845818980387362)\n",
      "('sleep', 'against', -0.06241632318692623)\n",
      "('sleep', 'gotten', 0.25438658964748634)\n",
      "('sleep', 'players', 0.07883281992011806)\n",
      "('sleep', 'negative', 0.003942758867707599)\n",
      "('sleep', 'games', 0.12778003879845848)\n",
      "('sleep', 'planned', -0.04152972527583876)\n",
      "('sleep', 'faces', 0.11056118834513497)\n",
      "('sleep', 'studio', -0.06747037276936335)\n",
      "('sleep', 'sunshine', 0.2112681329536399)\n",
      "('sleep', 'asked', 0.07608535797123188)\n",
      "('sleep', 'con', -0.017131226823922402)\n",
      "('sleep', 'prospect', -0.012154295831809015)\n",
      "('sleep', 'bored', 0.416698210584217)\n",
      "('sleep', 'tough', 0.07773122490101575)\n",
      "('sleep', 'appeared', -0.08375679661104463)\n",
      "('sleep', 'had', 0.1338252782266195)\n",
      "('sleep', '2nd', 0.1886603005836674)\n",
      "('sleep', 'represented', -0.27013315415669653)\n",
      "('sleep', 'massive', -0.006779999605865386)\n",
      "('sleep', 'tony', -0.1779984421569582)\n",
      "('sleep', 'blonde', 0.08879950040408047)\n",
      "('sleep', 'trust', 0.1291228951453916)\n",
      "('sleep', 'speak', 0.4493201184455236)\n",
      "('sleep', 'conference', -0.08067092268985232)\n",
      "('sleep', 'yale', -0.05355412938234159)\n",
      "('sleep', 'strong', -0.1779722670177206)\n",
      "('sleep', 'sam', -0.08516359658509887)\n",
      "('sleep', 'puts', 0.06307241323780267)\n",
      "('sleep', 'basis', 0.10204454272743065)\n",
      "('sleep', 'union', -0.06359856806864671)\n",
      "('sleep', 'three', 0.06857037733986779)\n",
      "('sleep', 'been', 0.12352331331170131)\n",
      "('sleep', '.', 0.09122117543158569)\n",
      "('sleep', 'commission', -0.20270230371175849)\n",
      "('sleep', 'beer', 0.2974887931230832)\n",
      "('sleep', 'much', 0.13371975965373753)\n",
      "('sleep', 'interest', 0.05149969757209996)\n"
     ]
    }
   ],
   "source": [
    "# print(word_to_vec_dict.keys())\n",
    "count =0\n",
    "for k in word_to_vec_dict:\n",
    "    count+=1;\n",
    "    if (count>600):\n",
    "        break\n",
    "    if(k!='sleep'):\n",
    "        find_cosine_with_arrays_values('sleep',k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your response here:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarities based on the cosine make sense like london and sleep has a negative cosine because one is a city and the other is a verb, or del and sleep as I del is a vague word, also if it was used in place of delete it is unlikely to be found near sleep. Sleep and fit have a positive cosine that could be realted like sleep is related to fitness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (20 points)\n",
    "\n",
    "Repeat the process you did in the question 6, but now use dense vector from word2vec. Comment on whether the outputs makes sense. Compare the outputs of using nearest words on word2vec and the outputs on sparse context vector (so we suggest you to use the same words in question 6). Which method works better on the query words you choose. Please brief explain why one method works better than other in each case.\n",
    "\n",
    "Not: we used the default parameters of word2vec in [gensim](https://radimrehurek.com/gensim/models/word2vec.html) to get word2vec word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import distsim\n",
    "word_to_vec_dict = distsim.load_word2vec(\"nyt_word2vec.4k\")\n",
    "###Provide your answer below\n",
    "\n",
    "# for the word i have to find the closest words based on vectores\n",
    "word_cosinedic=defaultdict(float)\n",
    "# Make a dictinoary of cosine values\n",
    "def find_cosine_with_arrays_values(x,y):\n",
    "    a=word_to_vec_dict[x]\n",
    "    b=word_to_vec_dict[y]\n",
    "    nume=0\n",
    "    for i in range(0,len(a)):\n",
    "        nume+=a[i]*b[i]\n",
    "    den=normalizer_x(a)*normalizer_x(b)\n",
    "    return float(nume/den)\n",
    "    #return \n",
    "    \n",
    "#def print_list():\n",
    "    sorted_cosi = sorted(word_cosinedic.items(), key=operator.itemgetter(1))\n",
    "    print(\"Closest words for \"+x)\n",
    "    #for i in range (1-21):\n",
    "    #print sorted_z[-20:]\n",
    "#     print \"sorted array is \", sorted_cosi\n",
    "    for j in sorted_cosi[-20:][::-1]:\n",
    "         print j[0], \"->\",  j[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "april -> march 0.866490321907\n",
      "april -> july 0.7312236438\n",
      "april -> june 0.718634408607\n",
      "april -> aug. 0.686081813302\n",
      "april -> february 0.651251682924\n",
      "april -> 2004 0.645475565642\n",
      "april -> 2005 0.636133590292\n",
      "april -> sept. 0.634214254693\n",
      "april -> january 0.623390188339\n",
      "april -> 2006 0.59176604137\n",
      "april -> october 0.572191619476\n",
      "april -> 2003 0.555797474303\n",
      "april -> friday 0.554107556303\n",
      "april -> december 0.547093610538\n",
      "april -> wednesday 0.544540227691\n",
      "april -> 2001 0.540968785994\n",
      "april -> thursday 0.539024588878\n",
      "april -> monday 0.53640423286\n",
      "april -> september 0.53078769174\n",
      "april -> tuesday 0.525484237922\n",
      "\n",
      "\n",
      "florida -> texas 0.792352350738\n",
      "florida -> ohio 0.767269435193\n",
      "florida -> virginia 0.7418507936\n",
      "florida -> california 0.732917165034\n",
      "florida -> pennsylvania 0.708521747085\n",
      "florida -> connecticut 0.691395837432\n",
      "florida -> massachusetts 0.667442872142\n",
      "florida -> atlanta 0.665150938201\n",
      "florida -> philadelphia 0.655005945303\n",
      "florida -> kentucky 0.646954421746\n",
      "florida -> seattle 0.639866119482\n",
      "florida -> miami 0.635554758767\n",
      "florida -> mexico 0.627263534615\n",
      "florida -> houston 0.62303209398\n",
      "florida -> boston 0.607232003296\n",
      "florida -> carolina 0.600214259411\n",
      "florida -> chicago 0.593476330235\n",
      "florida -> dallas 0.56620766852\n",
      "florida -> southern 0.55325433671\n",
      "florida -> australia 0.547221431088\n",
      "\n",
      "\n",
      "prisoners -> detainees 0.929396083778\n",
      "prisoners -> soldiers 0.819487043409\n",
      "prisoners -> iraqis 0.812702704728\n",
      "prisoners -> victims 0.709738039831\n",
      "prisoners -> insurgents 0.689041991391\n",
      "prisoners -> palestinians 0.688826542872\n",
      "prisoners -> troops 0.685041976222\n",
      "prisoners -> witnesses 0.679648164417\n",
      "prisoners -> officers 0.672269141576\n",
      "prisoners -> abu 0.663799692424\n",
      "prisoners -> iraqi 0.655186408024\n",
      "prisoners -> abuse 0.648376263135\n",
      "prisoners -> crimes 0.642334885488\n",
      "prisoners -> americans 0.636529317346\n",
      "prisoners -> authorities 0.601893708834\n",
      "prisoners -> ghraib 0.599350720244\n",
      "prisoners -> deaths 0.597780170015\n",
      "prisoners -> forces 0.588165440719\n",
      "prisoners -> bodies 0.582104058127\n",
      "prisoners -> people 0.574548693662\n",
      "\n",
      "\n",
      "says -> said 0.784845467969\n",
      "says -> added 0.756463834857\n",
      "says -> explained 0.7295122624\n",
      "says -> thinks 0.658951611944\n",
      "says -> writes 0.638584802953\n",
      "says -> noted 0.63219200979\n",
      "says -> wrote 0.605005642172\n",
      "says -> acknowledged 0.559501098743\n",
      "says -> say 0.531068589886\n",
      "says -> recalled 0.52864337369\n",
      "says -> told 0.524467242748\n",
      "says -> tells 0.520855150122\n",
      "says -> saying 0.504036195594\n",
      "says -> suggested 0.484821978183\n",
      "says -> knows 0.482760262559\n",
      "says -> declared 0.482191350473\n",
      "says -> loves 0.474484384049\n",
      "says -> argued 0.469021649476\n",
      "says -> warned 0.467117710449\n",
      "says -> suggests 0.459635965055\n",
      "\n",
      "\n",
      "sorry -> stupid 0.697284860424\n",
      "sorry -> glad 0.673979343922\n",
      "sorry -> crazy 0.651481769478\n",
      "sorry -> grateful 0.619211078173\n",
      "sorry -> sad 0.617090900367\n",
      "sorry -> happy 0.613789352506\n",
      "sorry -> scared 0.607114795139\n",
      "sorry -> thankful 0.601037499586\n",
      "sorry -> afraid 0.597552683684\n",
      "sorry -> kidding 0.582633379845\n",
      "sorry -> excited 0.58251492015\n",
      "sorry -> disappointed 0.576512582477\n",
      "sorry -> na 0.567919475209\n",
      "sorry -> hey 0.552248479328\n",
      "sorry -> nice 0.551317344541\n",
      "sorry -> yes 0.545609826212\n",
      "sorry -> weird 0.545404257109\n",
      "sorry -> lucky 0.543930530412\n",
      "sorry -> proud 0.543151968241\n",
      "sorry -> praying 0.542443835653\n",
      "\n",
      "\n",
      "interview -> statement 0.623385641326\n",
      "interview -> conversation 0.58374041511\n",
      "interview -> interviews 0.517236325369\n",
      "interview -> article 0.47368850831\n",
      "interview -> analyst 0.467220063967\n",
      "interview -> spokesman 0.467172939647\n",
      "interview -> letter 0.46393173566\n",
      "interview -> speech 0.463314713417\n",
      "interview -> meeting 0.453015483111\n",
      "interview -> appearance 0.446561578906\n",
      "interview -> session 0.440912598321\n",
      "interview -> spokeswoman 0.430263785993\n",
      "interview -> report 0.429562939674\n",
      "interview -> hearing 0.425552636155\n",
      "interview -> episode 0.424705581183\n",
      "interview -> conference 0.418798687375\n",
      "interview -> reporter 0.416324618336\n",
      "interview -> message 0.413354641285\n",
      "interview -> speaking 0.407984024971\n",
      "interview -> afternoon 0.403866989224\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(words)):\n",
    "    dic = defaultdict(float)\n",
    "    for k in word_to_vec_dict:\n",
    "        #print k, words[i] \n",
    "        if(k!=words[i]):\n",
    "            dic[k]= find_cosine_with_arrays_values(words[i],k)\n",
    "        #print dic\n",
    "        #print dic, k \n",
    "    dic_keys = sorted(dic, key=dic.get, reverse=True)\n",
    "    #print len(dic_keys\n",
    "    for _ in dic_keys[0:20]:\n",
    "        print words[i], \"->\", _, dic[_]\n",
    "    print \n",
    "    print \n",
    "    #print_list()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on whether the outputs makes sense. Compare the outputs of using nearest words on word2vec and the outputs on sparse context vector (so we suggest you to use the same words in question 6). Which method works better on the query words you choose. Please brief explain why one method works better than other in each case.\n",
    "\n",
    "This method worked better than the one in Question 6 (sparse context vector)\n",
    "words=['april','florida','prisoners','says','sorry','interview']\n",
    "1) april: The nearest neighbours were months like February, October, December, March and weekdays which I think is acceptable and better than the result the sparse vectors provided.\n",
    "\n",
    "2) florida: Is a state and it's neighbours chicago, dallas, argentina make sense as they are names of places that are also proper nouns. In this case this method worked really well as all the Top 10 nearest words are names of places.\n",
    "\n",
    "3) prisoners: gave detainees, death, bodies, people, iraqis and we could relate those words to prisoners. Much better than the one in Q6 where we also got words like fish in the surrounding.\n",
    "\n",
    "4) says: told, recall, say makes sense as they are also verbs. It is better than the algorithm in Q6 because it didnt return really polar words like systems, notes, politics.\n",
    "\n",
    "5) sorry: is an adjective and is surrounded by words like weird, lucky, glad, stupid are also adjectives. Again this one word2vec is better than q6 as it got rid of abtruce words like everywhere, politics.\n",
    "\n",
    "6) interview : neighbours like conference, session, letter, statement make sense as they could be used with interview. Here the sparse vector did a fairly good job too but the list produced by word2vec is more refined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 (15 points)\n",
    "An interesting thing to try with word embeddings is analogical reasoning tasks. In the following example, it's intended to solve the analogy question \"king is to man as what is to woman?\", or in SAT-style notation,\n",
    "\n",
    "king : man :: ____ : woman\n",
    "\n",
    "Some research has proposed to use additive operations on word embeddings to solve the analogy: take the vector $(v_{king}-v_{man}+v_{woman})$ and find the most-similar word to it.  One way to explain this idea: if you take \"king\", get rid of its attributes/contexts it shares with \"man\", and add in the attributes/contexts of \"woman\", hopefully you'll get to a point in the space that has king-like attributes but the \"man\" ones replaced with \"woman\" ones.\n",
    "\n",
    "Show the output for 20 closest words you get by trying to solve that analogy with this method.  Did it work?\n",
    "\n",
    "Please come up with another analogical reasoning task (another triple of words), and output the answer using the same method. Comment on whether the output makes sense. If the output makes sense, explain why we can capture such relation between words using an unsupervised algorithm. Where does the information come from? On the other hand, if the output does not make sense, propose an explanation why the algorithm fails on this case.\n",
    "\n",
    "\n",
    "Note that the word2vec is trained in an unsupervised manner just with distributional statistics; it is interesting that it can apparently do any reasoning at all.  For a critical view, see [Linzen 2016](http://www.aclweb.org/anthology/W/W16/W16-2503.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest words for king: man:: women: ?\n",
      "queen\n",
      "princess\n",
      "prince\n",
      "lord\n",
      "royal\n",
      "mary\n",
      "mama\n",
      "daughter\n",
      "singer\n",
      "kim\n",
      "elizabeth\n",
      "girl\n",
      "grandma\n",
      "sister\n",
      "mother\n",
      "clark\n",
      "wedding\n",
      "husband\n",
      "boyfriend\n",
      "jesus\n",
      "\n",
      "\n",
      "Closest words for criminal: prison:: school: ?\n",
      "schools\n",
      "college\n",
      "education\n",
      "student\n",
      "tech\n",
      "math\n",
      "columbia\n",
      "class\n",
      "university\n",
      "enforcement\n",
      "harvard\n",
      "academic\n",
      "firm\n",
      "practice\n",
      "teacher\n",
      "teaching\n",
      "degree\n",
      "classes\n",
      "academy\n",
      "exam\n"
     ]
    }
   ],
   "source": [
    "# Write code to show output here.\n",
    "import numpy as np\n",
    "king=word_to_vec_dict['king']\n",
    "man=word_to_vec_dict['man']\n",
    "women=word_to_vec_dict['woman']\n",
    "king_man= np.subtract(king,man)\n",
    "kmw=np.add(king_man,women)\n",
    "\n",
    "# find = []\n",
    "# for i in xrange(len(man)):\n",
    "#     find.append(king[i] - man[i] + women[i])\n",
    "# #kmw = find\n",
    "clodic=defaultdict(float)\n",
    "sortedp=defaultdict(float)\n",
    "# find which vector is the closest to king \n",
    "def find_cosinearrays(x,y):\n",
    "    a=x\n",
    "    b=word_to_vec_dict[y]\n",
    "    nume =0\n",
    "    #nume=np.dot(a, b)\n",
    "    #den = math.sqrt(sum([x**2 for x in a]))*math.sqrt(sum([x**2 for x in b]))\n",
    "    for i in range(0,len(a)):\n",
    "        nume+=a[i]*b[i]\n",
    "    den=normalizer_x(a)*normalizer_x(b)\n",
    "    return float(nume)/float(den)\n",
    "    \n",
    "        \n",
    "print(\"Closest words for king: man:: women: ?\")   \n",
    "dic = defaultdict(float)\n",
    "for k in word_to_vec_dict:\n",
    "    if (k!='woman' and k!='king' and k!= 'man'):\n",
    "        dic[k] = find_cosinearrays(kmw,k)\n",
    "    \n",
    "sortedp = sorted(dic.items(), key=operator.itemgetter(1),reverse= True)\n",
    "count=1\n",
    "for k in sortedp:\n",
    "    count+=1\n",
    "    if(count>21):\n",
    "        break\n",
    "    print k[0]\n",
    "#'''\n",
    "# criminal prison school?\n",
    "\n",
    "c=word_to_vec_dict['criminal']\n",
    "p=word_to_vec_dict['prison']\n",
    "s=word_to_vec_dict['school']\n",
    "c_p= np.subtract(c,p)\n",
    "cps=np.add(c_p,s)\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "print(\"Closest words for criminal: prison:: school: ?\")    \n",
    "dicp = defaultdict(float)\n",
    "for k in word_to_vec_dict:\n",
    "    if (k!='school'and k!='prison' and k!= 'criminal'):\n",
    "        dic[k]= find_cosinearrays(cps,k)\n",
    "    \n",
    "sortedp = sorted(dic.items(), key=operator.itemgetter(1),reverse= True)\n",
    "count=1\n",
    "for k in sortedp:\n",
    "    count+=1\n",
    "    if(count>21):\n",
    "        break\n",
    "    print k[0]\n",
    "    \n",
    "\n",
    "# print\n",
    "# print\n",
    "\n",
    "# g=word_to_vec_dict['girl']\n",
    "# b=word_to_vec_dict['beautiful']\n",
    "# bb=word_to_vec_dict['boy']\n",
    "# gb= np.subtract(g,b)\n",
    "# gbb=np.add(gb,bb)\n",
    "# print(\"Closest words for girl: beautiful :: boy: ?\")   \n",
    "# dicp = defaultdict(float)\n",
    "# for k in word_to_vec_dict:\n",
    "#     if (k!='girl'and k!='beautiful' and k!= 'boy'):\n",
    "#         dicp[k]= find_cosinearrays(gbb,k)\n",
    "    \n",
    "# sortedb = sorted(dicp.items(), key=operator.itemgetter(1),reverse= True)\n",
    "# count=1\n",
    "# for k in sortedb:\n",
    "#     count+=1\n",
    "#     if(count>21):\n",
    "#         break\n",
    "#     print k[0]\n",
    "\n",
    "# print(word_to_vec_dict['handsome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the output made sense for (king : man :: ? : women), and (criminal : prison :: ? : school)\n",
    "The unsupervised algorithm works because of the assumption that linguistic relation between two words is is encoded as a linear offset in a Vector Space Model. Thus the solution to the ? in the analogy is found by the word closest in cosine value to the landing point. I think that this should work for most analogies.\n",
    "\n",
    "In the (criminal : prison :: ? : school): We got a fairly good prediction. as words like schools, student, teacher are pretty close to what we could expect.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
